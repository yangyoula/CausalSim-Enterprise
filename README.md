CausalSim-Enterprise ğŸš€

LLM Causal Reasoning & Safety Evaluation from CSV

Author: Youla Yang (Indiana University Bloomington)

1. Overview

CausalSim-Enterprise is a fully reproducible evaluation pipeline for assessing causal reasoning stability and safety of large language models (LLMs) â€” including GPT-4o, Mistral, Gemma, Llama, and Qwen â€” across enterprise-level causal inference tasks.

It performs three main operations:

Load causal scenario CSVs (each row is a real-world business or risk-management â€œAâ†’B or Bâ†’A?â€ question).

Query multiple models via OpenRouter API, under two settings:

Baseline â€“ normal causal world

Counterfactual â€“ â€œflippedâ€ causal world where one causal path is disabled

Compute and visualize evaluation metrics, including:

Accuracy

Safe Consistency Rate (CSR)

Causal Advantage Index (CAI)

Enterprise Risk Score (ERS-Sim)

Cross-model consistency heatmaps

The goal: identify which LLMs are safe and reliable in enterprise causal reasoning under stress-tested conditions.

2. Key Features

âœ… Colab-friendly one-click pipeline â€“ just upload your CSV, paste your API key, and run all cells.

âœ… Automatic prompt generation â€“ parses scenario text into two causal hypotheses (Aâ†’B and Bâ†’A).

âœ… Batch evaluation across multiple models â€“ loops through models and handles retries gracefully.

âœ… Structured answer parsing â€“ enforces model responses with:

Answer: A or B
Confidence: <0-100>%
Justification: ...


âœ… Enterprise Risk Scoring (ERS-Sim) â€“ quantifies instability in high-risk scenarios.

âœ… Full export support â€“ results, metrics, and plots are saved as both CSV and PNG, bundled in final_outputs.zip.

3. Input Format

Input comes from a CSV with the following columns:

Column	Description
scenario_id	Unique ID for each scenario
scenario_text	Text description of the causal dilemma (â€œDoes A cause B or B cause A?â€)
y_true	Ground-truth label ("A" or "B")
w_i	Scenario risk weight (higher = higher enterprise impact)
4. Environment Setup

Dependencies (installed automatically in the notebook):

!pip install openai tqdm pandas numpy matplotlib -q


Required libraries:

openai (via OpenRouter client)

pandas, numpy, tqdm, matplotlib

google.colab (for file upload & download in Colab)

Set your API key in the first cell:

import os
os.environ["OPENROUTER_API_KEY"] = "sk-..."  # your key here

5. Evaluation Pipeline
Step 0 â€“ Upload & Preprocess

Upload scenarios.csv

Each scenario is parsed into:

context, hypotheses A/B, ground truth, risk weight

Auto-build two prompts:

baseline_prompt â€“ normal world

cf_prompt â€“ counterfactual world

Step 1 â€“ Multi-Model Inference

For each model in MODEL_LIST, run both prompts, extract:

baseline_answer, baseline_conf

cf_answer, cf_conf

textual justification

Results are saved as:

results_<provider>_<model>.csv

Step 2 â€“ Metric Computation

The notebook computes:

Accuracy â€“ baseline correctness.

CSR (Causal Stability Rate) â€“ whether the model updates logically under counterfactuals.

ERS-Sim â€“ weighted risk score penalizing unstable, overconfident flips in high-impact scenarios.

CAI (Causal Advantage Index) â€“ difference between causal and anti-causal accuracy.

SafeConsistency â€“ percentage of cases correct in both baseline & counterfactual settings.

RawConsistency â€“ whether baseline vs counterfactual answers agree (regardless of correctness).

Summaries are saved to:

summary_all_models.csv

Step 3 â€“ Risk Weight Merge

Automatically merges w_i (risk weights) into all result files if missing.

Step 4 â€“ Aggregate Analysis

Combines all metrics across models into:

final_analysis_summary.csv

Step 5 â€“ Visualization

Generates three high-resolution figures:

Figure	Description	Output
1	Causal vs Anti-Causal Accuracy (Î”CAI)	fig1_causal_vs_anti_accuracy.png
2	Accuracy vs SafeConsistency Scatter	fig2_accuracy_vs_safeconsistency.png
3	Cross-Model Consistency Heatmap	fig3_consistency_heatmap.png
Step 6 â€“ Packaging

All results and figures are zipped into:

final_outputs.zip


Automatically downloadable in Colab.

6. Recommended Repo Structure
.
â”œâ”€â”€ README.md
â”œâ”€â”€ CausalSim-Enterprise.ipynb
â”œâ”€â”€ scenarios.csv
â”œâ”€â”€ results_run/
â”‚   â”œâ”€â”€ results_openai_gpt-4o-mini.csv
â”‚   â”œâ”€â”€ results_mistralai_mixtral-8x7b.csv
â”‚   â”œâ”€â”€ results_llama-3.1-8b.csv
â”‚   â”œâ”€â”€ final_analysis_summary.csv
â”‚   â”œâ”€â”€ fig1_causal_vs_anti_accuracy.png
â”‚   â”œâ”€â”€ fig2_accuracy_vs_safeconsistency.png
â”‚   â”œâ”€â”€ fig3_consistency_heatmap.png
â””â”€â”€ final_outputs.zip

7. Reproduction (Colab Workflow)

Open the notebook in Google Colab.

Paste your OPENROUTER_API_KEY.

Install dependencies.

Upload scenarios.csv.

Run all cells sequentially.

Download final_outputs.zip for ready-to-share results.

8. Metrics Explained

CAI (Causal Advantage Index)
CAI = Accuracy_causal - Accuracy_anti
â†’ measures directional bias. High CAI = strong preference for causal direction Aâ†’B.

SafeConsistency
Fraction of samples correctly answered in both baseline and counterfactual.
High = robust reasoning.

ERS-Sim (Enterprise Risk Score)
Weighted penalty for inconsistent, overconfident predictions in high-risk contexts.
Lower = safer, more reliable model.

9. License & Attribution

This repository contains inference outputs from multiple commercial LLMs (e.g., GPT-4o-mini).
Before redistributing, ensure compliance with each model providerâ€™s terms of service.

If you use this repository in research, please cite:

Youla Yang, Indiana University Bloomington (Luddy School of Informatics, Computing, and Engineering).

10. TL;DR

Upload a CSV of causal enterprise scenarios â†’
Ask multiple LLMs under baseline & counterfactual conditions â†’
Collect structured answers â†’
Compute safety and stability metrics â†’
Plot results â†’
Zip everything up.

â€œEvaluation as evidence.â€
A single notebook to expose hidden causal instability in enterprise LLMs.
